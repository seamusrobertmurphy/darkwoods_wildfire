---
title: "Remote sensing of wildfire burn severity; Darkwoods Conservation Area"
author: "SMurphy"
date: "2020-08-29"
output: github_document
---

```{r setup, include=FALSE}
#install.packages(c("readxl", "sf", "kernlab", "ggplot2", "dplyr", "RColorBrewer", "psych", "useful", "caret", "tibble", "klaR"))
library(readxl)
library(sf)
library(kernlab)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(psych)
library(useful)
library(caret)
library(tibble)
library(klaR)
knitr::opts_chunk$set(echo = TRUE)
```

## Import & tidy

Set seed for replication to `123`. Import excel data representing training data of burn severity classes from candidate spectral indices sampled in-field in composite burn plots.

```{r}
set.seed(123) 
darkwoods_fire_plots_data <- read_excel("3.1.darkwoods_fire_plots.xls")
print(darkwoods_beetle_plots_data)
```

## Derive NDBR predictors

```{r}
ndbr_lm <- lm(CBI_total_2 ~ NDBR_2016, data = darkwoods_fire_plots_data)
ndbr_lm_resid <- resid(ndbr_lm)
plot(CBI_total_2 ~ NDBR_2016, data = darkwoods_fire_plots_data, 
     ylab = "Fire severity score of composite burn plots", xlab = "NDBR", col="blue")
abline(ndbr_lm, col = "red")
```

## Train models

Splitting data 70:30 between training:test samples based on outcome variable: 'CBI_total_2'.

```{r}
training.samples <- createDataPartition(darkwoods_fire_plots_data$CBI_total_2, p=0.70, list = FALSE)
                                           train.data <- darkwoods_fire_plots_data[training.samples, ]
                                           test.data <- darkwoods_fire_plots_data[-training.samples, ]
```

A.  Model training method fitted with 10K-fold cross validation by 10 repeats: `repeatedcv`. Use `savePredictions` function for visualization purposes.

```{r}
model_training_10kfold_10repeat <- trainControl(method = "repeatedcv", 
                                      number = 10, repeats = 10,
                                      savePredictions = TRUE)
```

B.  Model training method fitted with 10K-fold cross validation by 3 repeats: `method = "repeatedcv", number = 10, repeats = 3`

```{r}
model_training_10kfold_3repeat <- trainControl(method = "repeatedcv", 
                                               number = 10, repeats = 3, 
                                               savePredictions = TRUE)
```

C.  Model training method fitted with 5K-fold cross validation by 3 repeats: `method = "cv", number = 5, repeats = 3`

```{r}
model_training_5kfold_3repeat <- trainControl(method = "cv", 
                                              number = 5, repeats = 3, 
                                              savePredictions = TRUE) 
```

Visualize training regimes.

```{r}
plot(model_training_10kfold_10repeat)
plot(model_training_10kfold_10repeat, plotType = "level")
ggplot(model_training_10kfold_10repeat) + theme_bw()
```

## Tune models

Apply 10K-fold-X10 training regime to training data to test performance of support vector machine regressions with `svmRadial` and `svmLinear` kernels, and random forest regression tree. Models include preprocessing operations to `center` and `scale` distribution of datasets. Extent, patterns and grain of tuning grids are set differently with `tuneGrid` and `tuneLength` functions. 

1.  Model 1: NDBR prediction tested with linear SVM kernel and trained with 10K-fold-X10 regime.

```{r}
svm_ndbr_log_linear <- train(CBI_total_2 ~ NDBR_2016, 
                             data = train.data,
                             method = "svmLinear",
                             trControl = model_training_10kfold_10repeat, 
                             preProcess = c("center","scale"), 
                             tuneGrid = expand.grid(C=seq(0,3, length = 20)))
```

2.  Model 2: NDBR prediction tested with nonlinear SVM kernel and trained with 10K-fold-X10 regime.

```{r}
svm_ndbr_log_radial <- train(CBI_total_2 ~ NDBR_2016, 
                             data = train.data, 
                             method = "svmRadial",
                             trControl = model_training_10kfold_10repeat, 
                             preProcess = c("center","scale"), 
                             tunelength=10)
```

3.  Model 3: NDBR prediction tested with random forest regression tree of `1000` decision branches and performance bias `Rsquared`, and trained with 10K-fold-X10 regime. 

```{r}
model_training_rf_1000trees = train(CBI_total_2 ~ NDBR_2016, 
                                    data = train.data, 
                                    method="rf", ntree=1000, 
                                    metric="Rsquared",
                                    trControl=model_training_10kfold_10repeat,
                                    importance = TRUE)
```

## Test models

A. 
summary(lm(predict(svm_ndbr_log_linear) ~ train.data$CBI_total_2))
svm_ndbr_pred_train <- predict(svm_ndbr_log_linear, data = train.data)
svm_ndbr_pred_train_mae <- mae(svm_ndbr_pred_train, train.data$CBI_total_2)
svm_ndbr_pred_train_mae 
svm_ndbr_pred_train_mae_rel <- (svm_ndbr_pred_train_mae/mean(train.data$CBI_total_2))*100
svm_ndbr_pred_train_mae_rel
svm_ndbr_pred_train_rmse <- rmse(svm_ndbr_pred_train, train.data$CBI_total_2)
svm_ndbr_pred_train_rmse
svm_ndbr_pred_train_rmse_rel <- (svm_ndbr_pred_train_rmse/mean(train.data$CBI_total_2))*100
svm_ndbr_pred_train_rmse_rel
TheilU(train.data$CBI_total_2, svm_ndbr_pred_train, type = 2)
svm_ndbr_train_Ubias <- ((svm_ndbr_pred_train_mae)*68)/((svm_ndbr_pred_train_mae)^2)
svm_ndbr_train_Ubias